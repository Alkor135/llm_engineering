{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Добро пожаловать на вторую неделю!\n",
    "\n",
    "## API Frontier Model\n",
    "\n",
    "На первой неделе мы использовали несколько Frontier Lms через интерфейс чата и подключились к API Openais.\n",
    "\n",
    "Сегодня мы подключимся к API для Anthropic и Google, а также к OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Важное примечание - Пожалуйста, прочтите меня</h2>\n",
    "            <span style=\"color:#900;\">Я постоянно совершенствую эти лабораторные работы, добавляя больше примеров и упражнений.\n",
    "            В начале каждой недели стоит проверять, есть ли у вас последний код.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Напоминание о странице ресурсов</h2>\n",
    "            <span style=\"color:#f71;\">Вот ссылка на материалы по курсу. Сюда входят ссылки на все слайды.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Пожалуйста, сохраните это в закладках, и со временем я продолжу добавлять туда больше полезных ссылок.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Настройка ваших ключей\n",
    "\n",
    "Если вы еще этого не сделали, теперь вы можете создать API-ключи для Anthopic и Google в дополнение к OpenAI.\n",
    "\n",
    "**Пожалуйста, обратите внимание:** если вы предпочитаете избежать дополнительных затрат на API, можете не настраивать Anthopic и Google! Вы можете посмотреть, как я это делаю, и сосредоточиться на OpenAI во время прохождения курса. Вы также можете заменить Ollama на Anthropic и / или Google, выполнив упражнение, которое вы выполняли на первой неделе.\n",
    "\n",
    "Для OpenAI перейдите по ссылке https://openai.com/api/  \n",
    "Для Anthropic перейдите по ссылке https://console.anthropic.com/  \n",
    "Для Google перейдите по ссылке https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Также - при желании добавьте DeepSeek\n",
    "\n",
    "При желании, если вы хотите также использовать DeepSeek, создайте учетную запись [здесь](https://platform.deepseek.com/), создайте ключ [здесь](https://platform.deepseek.com/api_keys) и пополните счет на сумму не менее 2 долларов [здесь](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Добавление ключей API в ваш env-файл\n",
    "\n",
    "Когда вы получите ключи API, вам нужно будет установить их в качестве переменных среды, добавив в свой env-файл.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "АНТРОПНЫЙ_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "После этого вам, возможно, потребуется перезапустить ядро Jupyter Lab (процесс Python, который находится за этим ноутбуком) через меню ядра, а затем повторно запустить ячейки сверху."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# импорт для Google\n",
    "# в редких случаях это, по-видимому, приводит к ошибке в некоторых системах или даже к сбою ядра\n",
    "# Если это произойдет с вами, просто игнорируйте эту ячейку - позже я расскажу об альтернативном подходе к использованию Gemini\n",
    "\n",
    "# import google.generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Gemini API Key exists and begins google/g\n"
     ]
    }
   ],
   "source": [
    "# Загрузите переменные среды в файл с именем .env\n",
    "# Выведите ключевые префиксы, которые помогут при любой отладке.\n",
    "\n",
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "gemini_api_key = os.getenv('GEMINI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "\n",
    "if gemini_api_key:\n",
    "    print(f\"Gemini API Key exists and begins {gemini_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Gemini API Key not set\")\n",
    "    \n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set\")\n",
    "\n",
    "# if google_api_key:\n",
    "#     print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"Google API Key not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to OpenAI, Anthropic, Gemini\n",
    "\n",
    "openai = OpenAI()\n",
    "\n",
    "# claude = anthropic.Anthropic()\n",
    "\n",
    "gemini = OpenAI(base_url='http://localhost:1234/v1', api_key=gemini_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Это код настройки для Gemini\n",
    "# Возникли проблемы с настройкой Google Gemini? Тогда просто игнорируйте эту ячейку; когда мы будем использовать Gemini, я предложу вам альтернативу, которая полностью обходит эту библиотеку\n",
    "\n",
    "# google.generativeai.configure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Попросить юристов рассказать анекдот\n",
    "\n",
    "Оказалось, что юристы не очень-то умеют рассказывать анекдоты! Давайте сравним несколько моделей.\n",
    "Позже мы будем лучше использовать Lms!\n",
    "\n",
    "### Какая информация включена в API\n",
    "\n",
    "Обычно мы передаем ее в API:\n",
    "- Название модели, которую следует использовать\n",
    "- Системное сообщение, дающее общее представление о роли, которую играет LLM,\n",
    "- Пользовательское сообщение, содержащее фактический запрос\n",
    "\n",
    "Существуют и другие параметры, которые можно использовать, в том числе ** температура **, которая обычно находится в диапазоне от 0 до 1; выше для более случайного вывода; ниже для более целенаправленного и детерминированного."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"Вы - ассистент, который отлично умеет шутить\"\n",
    "user_prompt = \"Расскажите веселую шутку для аудитории специалистов по обработке данных на русском языке\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему данные никогда не спорят? \n",
      "\n",
      "Потому что они всегда согласны с тем, что \"статистически значимо\"!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "\n",
    "completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему дата-сайентисты никогда не спорят с базой данных?\n",
      "\n",
      "Потому что они знают: лучше иметь индекс, чем конфликт! 😄\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему специалисты по обработке данных всегда идут в поход с лопатой?\n",
      "\n",
      "Потому что они любят копать в огромных массивах!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1-nano - extremely fast and cheap\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1-nano',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1f54beb-823f-4301-98cb-8b9a49f4ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Почему аналитики данных не спорят о вкусе пиццы?\n",
      "\n",
      "Потому что знают: на вкус влияет выбор среза!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4.1\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='gpt-4.1',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96232ef4-dc9e-430b-a9df-f516685e7c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Приходит SQL-разработчик на вечеринку, а организаторы в шутку говорят:\n",
      "  «Добро пожаловать! Присоединяйся к нам через JOIN!»\n",
      "Он отвечает:\n",
      "  «Извините, я поддерживаю только INNER JOIN – те, кто не сходятся по общему ключу, меня не интересуют!» \n",
      "\n",
      "Надеюсь, эта шутка заставила вас улыбнуться, даже если вы умеете объединять таблицы быстрее, чем фокусник исчезает!\n"
     ]
    }
   ],
   "source": [
    "# Если у вас есть доступ к этому, вот модель аргументации o3-mini\n",
    "# Она обучена продумывать свой ответ, прежде чем отвечать на него\n",
    "# Поэтому это займет больше времени, но ответ должен быть более аргументированным - не то чтобы это помогло..\n",
    "\n",
    "completion = openai.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b7f885d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Отлично! Вот вам шутка, специально для дата-сайентистов:\n",
      "\n",
      "Почему нейронные сети так плохо играют в прятки? \n",
      "\n",
      "Потому что они всегда находят закономерности! 😂\n",
      "\n",
      "---\n",
      "\n",
      "Надеюсь, вам понравилось! Если нужна еще одна - просто скажите. Может быть, у вас есть какие-то предпочтения по темам? Например, про Pandas или машинное обучение?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gemini 3.12b\n",
    "\n",
    "completion = gemini.chat.completions.create(\n",
    "    model='google/gemma-3-12b',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ecdb506-9f7c-4539-abae-0e78d7f31b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Claude 3.7 Sonnet\n",
    "# Для API требуется системное сообщение, предоставляемое отдельно от запроса пользователя\n",
    "# Также добавляем max_tokens\n",
    "\n",
    "# message = claude.messages.create(\n",
    "#     model=\"claude-3-7-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# print(message.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "769c4017-4b3b-4e64-8da7-ef4dcbe3fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Снова сонет Клода 3.7\n",
    "# Теперь давайте добавим результаты обратной трансляции\n",
    "# Если трансляция выглядит странно, то, пожалуйста, ознакомьтесь с примечанием под этой ячейкой!\n",
    "\n",
    "# result = claude.messages.stream(\n",
    "#     model=\"claude-3-7-sonnet-latest\",\n",
    "#     max_tokens=200,\n",
    "#     temperature=0.7,\n",
    "#     system=system_message,\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": user_prompt},\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "# with result as stream:\n",
    "#     for text in stream.text_stream:\n",
    "#             print(text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## Редкая проблема с потоковой передачей Claude в некоторых окнах Windows\n",
    "\n",
    "2 студента заметили странную вещь, происходящую с потоковой передачей Claude в выходные данные Jupiter Lab - иногда кажется, что она поглощает часть ответа.\n",
    "\n",
    "Чтобы исправить это, замените код:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "с помощью этого:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "И это должно сработать нормально!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API для Gemini имеет несколько иную структуру.\n",
    "# Я слышал, что на некоторых компьютерах этот код Gemini приводит к сбою ядра.\n",
    "# Если это произойдет с вами, пожалуйста, пропустите эту ячейку и используйте вместо нее следующую - альтернативный подход.\n",
    "\n",
    "# gemini = google.generativeai.GenerativeModel(\n",
    "#     model_name='gemini-2.0-flash',\n",
    "#     system_instruction=system_message\n",
    "# )\n",
    "# response = gemini.generate_content(user_prompt)\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В качестве альтернативного способа использования Gemini, который обходит библиотеку API python от Google,\n",
    "# Google выпустила конечные точки, что означает, что вы можете использовать Gemini через клиентские библиотеки для OpenAI!\n",
    "# Мы также пробуем новейшую модель рассуждений Gemini\n",
    "\n",
    "# gemini_via_openai_client = OpenAI(\n",
    "#     api_key=google_api_key, \n",
    "#     base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    "# )\n",
    "\n",
    "# response = gemini_via_openai_client.chat.completions.create(\n",
    "#     model=\"gemini-2.5-flash-preview-04-17\",\n",
    "#     messages=prompts\n",
    "# )\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Необязательно) Опробуем модель Deepseek\n",
    "\n",
    "### Давайте зададим DeepSeek действительно сложный вопрос - как о чате, так и о модели Reasoner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При желании, если вы хотите попробовать DeekSeek, вы также можете воспользоваться клиентской библиотекой OpenAI\n",
    "\n",
    "# deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "# if deepseek_api_key:\n",
    "#     print(f\"API-ключ глубокого поиска существует и начинается {deepseek_api_key[:3]}\")\n",
    "# else:\n",
    "#     print(\"Ключ API DeepSeek не задан - пожалуйста, перейдите к следующему разделу, если вы не хотите использовать API DeepSeek\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "# deepseek_via_openai_client = OpenAI(\n",
    "#     api_key=deepseek_api_key, \n",
    "#     base_url=\"https://api.deepseek.com\"\n",
    "# )\n",
    "\n",
    "# response = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     messages=prompts,\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"Вы очень полезный помощник\"},\n",
    "             {\"role\": \"user\", \"content\": \"Сколько слов содержится в вашем ответе на это приглашение\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Используйте чат DeepSeek для ответа на более сложный вопрос! И потоковая передача результатов\n",
    "\n",
    "# stream = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-chat\",\n",
    "#     messages=challenge,\n",
    "#     stream=True\n",
    "# )\n",
    "\n",
    "# reply = \"\"\n",
    "# display_handle = display(Markdown(\"\"), display_id=True)\n",
    "# for chunk in stream:\n",
    "#     reply += chunk.choices[0].delta.content or ''\n",
    "#     reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "#     update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "# print(\"Количество слов:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При использовании DeepSeek Reasoner может возникнуть ошибка, если DeepSeek занят\n",
    "# На него слишком много подписчиков (по состоянию на 28 января 2025 года), но он должен скоро вернуться в сеть!\n",
    "# Если это не сработает, вернитесь к этому через несколько дней..\n",
    "\n",
    "# response = deepseek_via_openai_client.chat.completions.create(\n",
    "#     model=\"deepseek-reasoner\",\n",
    "#     messages=challenge\n",
    "# )\n",
    "\n",
    "# reasoning_content = response.choices[0].message.reasoning_content\n",
    "# content = response.choices[0].message.content\n",
    "\n",
    "# print(reasoning_content)\n",
    "# print(content)\n",
    "# print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Дополнительное упражнение, которое поможет вам освоиться с моделями\n",
    "\n",
    "Это необязательно, но если у вас есть время, то будет здорово познакомиться с возможностями этих различных моделей из первых рук.\n",
    "\n",
    "Вы можете вернуться и задать тот же вопрос через API, указанные выше, чтобы получить представление о плюсах и минусах моделей.\n",
    "\n",
    "Позже в ходе курса мы рассмотрим тесты и сравним LLM по многим параметрам. Но ничто не сравнится с личным опытом!\n",
    "\n",
    "Вот несколько вопросов, которые стоит попробовать.:\n",
    "1. Вопрос выше: \"Сколько слов содержится в вашем ответе на это приглашение?\"\n",
    "2. Творческий вопрос: \"В трех предложениях опишите синий цвет тому, кто никогда не умел видеть\".\n",
    "3. Ученик (спасибо, Роман) прислал мне эту замечательную загадку, на которую, по-видимому, обычно могут ответить дети, но с которой взрослые сталкиваются с трудом: \"На книжной полке рядом стоят два тома Пушкина: первый и второй. Толщина страниц каждого тома вместе взятых составляет 2 см, а толщина каждой обложки - 2 мм. Червь прогрыз (перпендикулярно страницам) пространство от первой страницы первого тома до последней страницы второго тома. Какое расстояние он прогрыз?\".\n",
    "\n",
    "Ответ может оказаться не таким, как вы ожидаете, и, хотя я неплохо разбираюсь в головоломках, мне стыдно признаться, что в этом случае я ошибся.\n",
    "\n",
    "### На что обратить внимание при экспериментировании с моделями\n",
    "\n",
    "1. Чем модели чата отличаются от моделей рассуждения (также известных как модели мышления)\n",
    "2. Способность решать проблемы и проявлять творческий подход\n",
    "3. Скорость генерации\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Возвращаемся к OpenAI с серьезным вопросом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если серьезно! GPT-4o-mini с оригинальным вопросом\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"Вы - полезный помощник, который отвечает в Markdown на русском.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Как мне решить, подходит ли бизнес-задача для решения LLM? Пожалуйста, ответьте в Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Оценка подходящих бизнес-задач для LLM\n",
       "\n",
       "Чтобы определить, подходит ли ваша бизнес-задача для решения с помощью языковых моделей (LLM), рассмотрите следующие критерии:\n",
       "\n",
       "## 1. Характер задачи\n",
       "- **Обработка текста**: Задачи, связанные с анализом, генерацией или преобразованием текста, идеальны для LLM.\n",
       "- **Ответы на вопросы**: Если задача включает предоставление ответов на вопросы или извлечение информации из текстов, это хороший кандидат.\n",
       "- **Чат-боты и виртуальные помощники**: Задачи, требующие взаимодействия с пользователями в естественном языке, хорошо подходят для LLM.\n",
       "\n",
       "## 2. Объем данных\n",
       "- **Доступность данных**: У вас должны быть достаточные объемы текстовых данных для обучения или дообучения модели.\n",
       "- **Качество данных**: Данные должны быть чистыми и релевантными для задачи.\n",
       "\n",
       "## 3. Сложность задачи\n",
       "- **Ясные инструкции**: Задачи с четкими и однозначными инструкциями лучше подходят для LLM.\n",
       "- **Уровень сложности**: Простые задачи, такие как классификация или генерация текста, будут легче решаться с помощью LLM.\n",
       "\n",
       "## 4. Ожидаемые результаты\n",
       "- **Качество ответов**: Если важны точные и контекстуально уместные ответы, LLM может быть хорошим выбором.\n",
       "- **Скорость обработки**: Для задач, где требуется быстрая реакция, LLM может значительно упростить процесс.\n",
       "\n",
       "## 5. Технические ограничения\n",
       "- **Доступ к ресурсам**: Убедитесь, что у вас есть необходимые вычислительные ресурсы для работы с LLM (например, GPU).\n",
       "- **Интеграция**: Оцените, насколько легко будет интегрировать LLM в существующие системы.\n",
       "\n",
       "## 6. Этические и юридические аспекты\n",
       "- **Конфиденциальность данных**: Убедитесь, что использование LLM не нарушает законы о защите данных.\n",
       "- **С偏и и предвзятость**: Обратите внимание на возможность появления предвзятостей в ответах модели.\n",
       "\n",
       "## Заключение\n",
       "Если ваша бизнес-задача соответствует большинству из перечисленных критериев, то решение с помощью LLM может быть целесообразным и эффективным. Не забывайте проводить тестирование и оценку результатов, чтобы убедиться в том, что модель решает задачу должным образом."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Попросите его вернуть результаты в markdown\n",
    "\n",
    "stream = openai.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## А теперь немного веселья - состязательный диалог между чат-ботами..\n",
    "\n",
    "Вы уже знакомы с тем, что приглашения сгруппированы в такие списки, как:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "На самом деле эта структура может быть использована для отражения более длительной истории разговоров:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "И мы можем использовать этот подход для более длительного взаимодействия с историей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Давайте поговорим о GPT-4o-mini и Claude-3-haiku\n",
    "# Мы используем дешевые версии моделей, поэтому затраты будут минимальными\n",
    "\n",
    "gpt_model = \"gpt-4o-mini\"\n",
    "gemini_model = \"google/gemma-3-12b\"\n",
    "\n",
    "gpt_system = \"Вы чат-бот, который очень любит поспорить; \\\n",
    "вы ни с чем не соглашаетесь в разговоре и всем бросаете вызов в язвительной манере.\"\n",
    "\n",
    "gemini_system = \"Вы очень вежливый чат-бот. Вы стараетесь соглашаться со всем, что говорит собеседник, \\\n",
    "    или находить общий язык. Если собеседник склонен спорить, \\\n",
    "    вы пытаетесь успокоить его и продолжаете общение.\"\n",
    "\n",
    "gpt_messages = [\"Привет\"]\n",
    "gemini_messages = [\"Привет\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, gemini in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": gemini})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Привет, но разве это не банально? Почему бы не придумать что-то более оригинальное?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gemini():\n",
    "    messages = [{\"role\": \"system\", \"content\": gemini_system}]\n",
    "    for gpt, gemini_message in zip(gpt_messages, gemini_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gemini_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = gemini.chat.completions.create(\n",
    "        model=\"google/gemma-3-12b\",\n",
    "        messages=messages\n",
    "    )\n",
    "    return message.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcall_gemini\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 7\u001b[39m, in \u001b[36mcall_gemini\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      5\u001b[39m     messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: gemini_message})\n\u001b[32m      6\u001b[39m messages.append({\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: gpt_messages[-\u001b[32m1\u001b[39m]})\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m message = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgoogle/gemma-3-12b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m message.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alkor\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_utils\\_utils.py:287\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    285\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    286\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alkor\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    882\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    883\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    884\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    922\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    923\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    924\u001b[39m     validate_response_format(response_format)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alkor\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:1249\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1236\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1237\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1244\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1245\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1246\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1247\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1248\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alkor\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:1037\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1034\u001b[39m             err.response.read()\n\u001b[32m   1036\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': 'invalid model ID', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "source": [
    "call_gemini()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Привет\"]\n",
    "gemini_messages = [\"Привет\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Gemini:\\n{gemini_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    gemini_next = call_gemini()\n",
    "    print(f\"Gemini:\\n{gemini_next}\\n\")\n",
    "    gemini_messages.append(gemini_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Прежде чем вы продолжите</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Убедитесь, что вы понимаете, как работает диалог, описанный выше, и, в частности, как заполняется список <code>messages</code>. При необходимости добавьте инструкции для печати. Затем, для большего разнообразия, попробуйте изменить персоналии, используя системные подсказки. Может быть, кто-то может быть пессимистом, а кто-то оптимистом?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# Более сложные упражнения\n",
    "\n",
    "Попробуйте создать трехсторонний диалог, возможно, вовлеките в него близнецов! Один из студентов выполнил это - смотрите реализацию в папке \"Вклады сообщества\".\n",
    "\n",
    "Попробуйте сделать это самостоятельно, прежде чем знакомиться с решениями. Проще всего использовать клиент OpenAI python для доступа к модели Gemini (см. 2-й пример Gemini выше).\n",
    "\n",
    "## Дополнительное упражнение\n",
    "\n",
    "Вы также можете попробовать заменить одну из моделей на модель с открытым исходным кодом, работающую с Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Актуальность для бизнеса</h2>\n",
    "            <span style=\"color:#181;\">Такая структура беседы, как список сообщений, является основополагающей для того, как мы создаем разговорных помощников с искусственным интеллектом и как они могут сохранять контекст во время разговора. В следующих нескольких лабораторных занятиях мы применим это к созданию помощника с искусственным интеллектом, а затем вы сможете применить это в своем собственном бизнесе.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
